{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class FanficDataset(Dataset):\n",
    "    def __init__(self, path, probability=0.6):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.path = path\n",
    "        self.probability = probability\n",
    "        self.folders = os.listdir(path)\n",
    "        self.max_words = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        #total_files = 0\n",
    "        #for folder in self.folders:\n",
    "          #  total_files += len(os.listdir(os.path.join(self.path, folder)))\n",
    "        return len(self.folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_id = self.folders[idx]\n",
    "        folder_id_2= self.folders[idx]\n",
    "        random_number= np.random.random()\n",
    "        ## get a random file from the folder\n",
    "        files = os.listdir(os.path.join(self.path, folder_id))\n",
    "        file1 = files[np.random.randint(0, len(files))]\n",
    "        text1, text2 = None, None\n",
    "        similarity = 0\n",
    "        if random_number < self.probability:                \n",
    "            file2 = files[np.random.randint(0, len(files))]\n",
    "            similarity = 1\n",
    "        else:\n",
    "            random_number_2= np.random.randint(0, len(self.folders))\n",
    "            folder_id_2 = self.folders[random_number_2]\n",
    "            files_2 = os.listdir(os.path.join(self.path, folder_id_2))\n",
    "            file2 = files_2[np.random.randint(0, len(files_2))]\n",
    "            if folder_id == folder_id_2:\n",
    "                similarity = 1\n",
    "            else:\n",
    "                similarity = 0\n",
    "        with open(self.path + folder_id +\"/\"+file1, \"r\", encoding=\"utf-8\") as f:\n",
    "            text1 = f.read()\n",
    "            # Transform the text into lowercase\n",
    "            text1 = text1.lower()\n",
    "            # Split the text into words\n",
    "            words = text1.split()\n",
    "            start = np.random.randint(0, len(words) - self.max_words)\n",
    "            words = words[start:start + self.max_words]\n",
    "            input_ids1 = self.tokenizer.encode(words, add_special_tokens=True)\n",
    "\n",
    "        with open(self.path + folder_id_2 + \"/\" + file2, \"r\", encoding=\"utf-8\") as f:\n",
    "            text2 = f.read()\n",
    "            # Transform the text into lowercase\n",
    "            text2 = text2.lower()\n",
    "            # Split the text into words\n",
    "            words = text2.split()\n",
    "            start = np.random.randint(0, len(words) - self.max_words)\n",
    "            words = words[start:start + self.max_words]\n",
    "            input_ids2 = self.tokenizer.encode(words, add_special_tokens=True)\n",
    "        return input_ids1, input_ids2, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FanficDataset(\"Dataset/Fanfic/problem-n-full-10000/problem-n-full-10000/\")\n",
    "x,y,s = data[0]\n",
    "for i in range(1000):\n",
    "    print(data[i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
